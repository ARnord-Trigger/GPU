#include <stdio.h>
#include <stdlib.h>
#include <cuda_runtime.h>

#define N (1024 * 1024)
#define FULL_DATA_SIZE (N * 20)
#define CHECK(call)                                           \
{                                                             \
    const cudaError_t error = call;                           \
    if (error != cudaSuccess) {                               \
        fprintf(stderr, "Error: %s:%d\n", __FILE__, __LINE__);\
        fprintf(stderr, "Code: %d, Reason: %s\n",            \
                error, cudaGetErrorString(error));            \
        exit(1);                                              \
    }                                                         \
}

__global__ void kernel(int *a, int *b, int *c) {
    int idx = threadIdx.x + blockIdx.x * blockDim.x;
    if (idx < N) {
        c[idx] = (a[idx] + b[idx]) / 2;
    }
}

int main(void) {
    cudaDeviceProp prop;
    int whichDevice;
    CHECK(cudaGetDevice(&whichDevice));
    CHECK(cudaGetDeviceProperties(&prop, whichDevice));

    if (!prop.deviceOverlap) {
        printf("Device does not support overlap, no speedup from streams.\n");
        return 0;
    }

    cudaEvent_t start, stop;
    float elapsedTime;
    cudaStream_t stream;
    int *host_a, *host_b, *host_c;
    int *dev_a, *dev_b, *dev_c;

    // Start timers
    CHECK(cudaEventCreate(&start));
    CHECK(cudaEventCreate(&stop));

    // Initialize the stream
    CHECK(cudaStreamCreate(&stream));

    // Allocate memory on the GPU
    CHECK(cudaMalloc((void **)&dev_a, N * sizeof(int)));
    CHECK(cudaMalloc((void **)&dev_b, N * sizeof(int)));
    CHECK(cudaMalloc((void **)&dev_c, N * sizeof(int)));

    // Allocate host pinned memory
    CHECK(cudaHostAlloc((void **)&host_a, FULL_DATA_SIZE * sizeof(int), cudaHostAllocDefault));
    CHECK(cudaHostAlloc((void **)&host_b, FULL_DATA_SIZE * sizeof(int), cudaHostAllocDefault));
    CHECK(cudaHostAlloc((void **)&host_c, FULL_DATA_SIZE * sizeof(int), cudaHostAllocDefault));

    // Initialize host memory
    for (int i = 0; i < FULL_DATA_SIZE; i++) {
        host_a[i] = rand();
        host_b[i] = rand();
    }

    CHECK(cudaEventRecord(start, 0));

    // Loop over full data in chunks
    for (int i = 0; i < FULL_DATA_SIZE; i += N) {
        // Copy memory from host to device asynchronously
        CHECK(cudaMemcpyAsync(dev_a, host_a + i, N * sizeof(int), cudaMemcpyHostToDevice, stream));
        CHECK(cudaMemcpyAsync(dev_b, host_b + i, N * sizeof(int), cudaMemcpyHostToDevice, stream));

        // Launch kernel
        kernel<<<N / 256, 256, 0, stream>>>(dev_a, dev_b, dev_c);

        // Copy results back to host asynchronously
        CHECK(cudaMemcpyAsync(host_c + i, dev_c, N * sizeof(int), cudaMemcpyDeviceToHost, stream));
    }

    // Synchronize the stream
    CHECK(cudaStreamSynchronize(stream));
    CHECK(cudaEventRecord(stop, 0));
    CHECK(cudaEventSynchronize(stop));

    CHECK(cudaEventElapsedTime(&elapsedTime, start, stop));
    printf("The single stream with ID %d took %8.6f ms for data transfer and computation.\n", stream, elapsedTime);

    // Cleanup
    CHECK(cudaFreeHost(host_a));
    CHECK(cudaFreeHost(host_b));
    CHECK(cudaFreeHost(host_c));
    CHECK(cudaFree(dev_a));
    CHECK(cudaFree(dev_b));
    CHECK(cudaFree(dev_c));
    CHECK(cudaStreamDestroy(stream));

    return 0;
}
